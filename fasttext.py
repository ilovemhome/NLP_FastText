from gensim.models import FastText
from gensim.utils import simple_preprocess
import pandas as pd
import numpy as np
import urllib.request
import time
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
import random
warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_ag_news():
    train_url = "./train.csv"
    test_url = "./test.csv"
    
    train_df = pd.read_csv(train_url, header=None, names=['label', 'title', 'text'])
    test_df = pd.read_csv(test_url, header=None, names=['label', 'title', 'text'])
    
    print("\nğŸ“ æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ†è¯é¢„å¤„ç†...")
    train_corpus = (train_df['title'] + " " + train_df['text']).tolist()
    test_corpus = (test_df['title'] + " " + test_df['text']).tolist()
    train_labels = train_df['label'].tolist()
    test_labels = test_df['label'].tolist()
    
    train_tokens = []
    for text in tqdm(train_corpus, desc="è®­ç»ƒé›†åˆ†è¯"):
        train_tokens.append(simple_preprocess(text))
    test_tokens = []
    for text in tqdm(test_corpus, desc="æµ‹è¯•é›†åˆ†è¯"):
        test_tokens.append(simple_preprocess(text))
    
    combined = list(zip(train_tokens, train_labels))
    random.seed(42)
    random.shuffle(combined)
    train_tokens[:], train_labels[:] = zip(*combined)

    #print("\n=== æ•°æ®æ ‡ç­¾æ£€æŸ¥ ===")
    #print("è®­ç»ƒé›†æ ‡ç­¾çš„æ•°æ®ç±»å‹:", type(train_labels[0]))
    #print("è®­ç»ƒé›†æ ‡ç­¾æ ·ä¾‹:", train_labels[:10])
    #print("è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    #from collections import Counter
    #print(Counter(train_labels))
    #print("=== æ£€æŸ¥å®Œæ¯• ===\n")

    return train_tokens, train_labels, test_tokens, test_labels


class FastTextTrainingMonitor:
    def __init__(self, epochs):
        self.epochs = epochs
        self.epoch_progress = tqdm(total=epochs, desc="FastTextè®­ç»ƒè¿›åº¦", unit='epoch')
    
    def on_epoch_end(self):
        self.epoch_progress.update(1)
        self.epoch_progress.set_postfix({"è®­ç»ƒçŠ¶æ€": "æ­£å¸¸è¿›è¡Œä¸­"})
    
    def close(self):
        self.epoch_progress.close()

train_tokens, train_labels, test_tokens, test_labels = load_ag_news()

monitor = FastTextTrainingMonitor(epochs=25)

print("\nğŸš€ å¼€å§‹è®­ç»ƒFastTextæ¨¡å‹...")

model = FastText(
    vector_size=100, 
    window=5, 
    min_count=1, 
    sg=1,
    min_n=3, 
    max_n=6, 
    workers=1, 
    seed=42
)

model.build_vocab(corpus_iterable=train_tokens)

for epoch in range(25):
    model.train(
        corpus_iterable=train_tokens,  
        total_examples=model.corpus_count,
        epochs=1
    )
    monitor.on_epoch_end()  
monitor.close()

def get_text_vector(tokens, model):
    vecs = [model.wv[token] for token in tokens if token in model.wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)

print("\nğŸ”¢ æ­£åœ¨ç”Ÿæˆæ–‡æœ¬å‘é‡...")
train_vectors = []
for tokens in tqdm(train_tokens, desc="è®­ç»ƒé›†å‘é‡ç”Ÿæˆ"):
    train_vectors.append(get_text_vector(tokens, model))
train_vectors = np.array(train_vectors)

test_vectors = []
for tokens in tqdm(test_tokens, desc="æµ‹è¯•é›†å‘é‡ç”Ÿæˆ"):
    test_vectors.append(get_text_vector(tokens, model))
test_vectors = np.array(test_vectors)

print("\nğŸ§  è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨...")
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)

with tqdm(total=1, desc="åˆ†ç±»å™¨è®­ç»ƒ") as pbar:
    clf.fit(train_vectors, train_labels)
    pbar.update(1)

test_acc = clf.score(test_vectors, test_labels)
print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{test_acc:.4f}")

plt.figure(figsize=(10, 6))

test_texts_en = [
    "AI model improves natural language processing efficiency",
    "Chinese football team beats South Korea to qualify for Asian Cup",
    "Central bank cuts interest rates to boost stock market",
    "UN adopts resolution on climate cooperation"
]
test_texts_cn = [
    "AIæ¨¡å‹æå‡è‡ªç„¶è¯­è¨€å¤„ç†æ•ˆç‡",
    "å›½è¶³å‡»è´¥éŸ©å›½é˜Ÿæ™‹çº§äºšæ´²æ¯",
    "å¤®è¡Œé™æ¯ææŒ¯è‚¡å¸‚ä¸Šæ¶¨",
    "è”åˆå›½é€šè¿‡æ°”å€™åˆä½œå†³è®®"
]

label_mapping = {1: "World", 2: "Sports", 3: "Business", 4: "Tech"}
pred_results = []
pred_confidence = []  

for text in test_texts_en:
    tokens = simple_preprocess(text)
    vec = get_text_vector(tokens, model)
    pred_label = clf.predict([vec])[0]
    pred_proba = clf.predict_proba([vec])[0]
    pred_results.append(label_mapping[pred_label])
    pred_confidence.append(max(pred_proba))  

bars = plt.bar(
    range(len(test_texts_en)), 
    pred_confidence, 
    color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'],
    alpha=0.8
)

plt.xticks(
    range(len(test_texts_cn)), 
    [f"ç¤ºä¾‹{i+1}\n{text[:10]}..." for i, text in enumerate(test_texts_cn)], 
    rotation=15,
    fontsize=10
)
plt.ylabel('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)
plt.title('FastTextæ–‡æœ¬åˆ†ç±»é¢„æµ‹ç»“æœ', fontsize=14)
plt.ylim(0, 1.1)
plt.grid(axis='y', alpha=0.3)

for i, (bar, res, conf) in enumerate(zip(bars, pred_results, pred_confidence)):
    plt.text(
        bar.get_x() + bar.get_width()/2, 
        bar.get_height() + 0.02, 
        f"{res}\n{conf:.2f}", 
        ha='center', 
        va='bottom', 
        fontsize=11,
        fontweight='bold'
    )

plt.text(
    0.5, 1.05, 
    f"æ¨¡å‹æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡ï¼š{test_acc:.4f}", 
    ha='center', 
    fontsize=12,
    bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.7)
)

plt.tight_layout()
plt.savefig('fasttext_final_result.png', dpi=300, bbox_inches='tight')
print("\nâœ… å¯è§†åŒ–ç»“æœå›¾å·²ä¿å­˜ä¸ºï¼šfasttext_final_result.png")
plt.show()

print("\n===== ğŸ“‹ è¯¦ç»†é¢„æµ‹ç»“æœ =====")
for i, (text_cn, text_en, res, conf) in enumerate(zip(test_texts_cn, test_texts_en, pred_results, pred_confidence)):
    print(f"ã€ç¤ºä¾‹{i+1}ã€‘")
    print(f"ä¸­æ–‡æ–‡æœ¬ï¼š{text_cn}")
    print(f"è‹±æ–‡æ–‡æœ¬ï¼š{text_en}")
    print(f"é¢„æµ‹ç±»åˆ«ï¼š{res}")
    print(f"é¢„æµ‹ç½®ä¿¡åº¦ï¼š{conf:.4f}\n")

#print("\n=== è¯å‘é‡è´¨é‡è¯Šæ–­ ===")
#try:
    #print("ä¸ 'football' æœ€ç›¸ä¼¼çš„è¯:")
    #print(model.wv.most_similar('football', topn=5))
    #print("\nä¸ 'economy' æœ€ç›¸ä¼¼çš„è¯:")
    #print(model.wv.most_similar('economy', topn=5))
    #print("\nä¸ 'technology' æœ€ç›¸ä¼¼çš„è¯:")
    #print(model.wv.most_similar('technology', topn=5))
#except KeyError as e:
    #print(f"é”™è¯¯ï¼šè¯æ±‡ '{e}' ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ–‡æœ¬é¢„å¤„ç†æ—¶è¢«è¿‡æ»¤æ‰äº†ã€‚")
#print("=== è¯Šæ–­å®Œæ¯• ===")